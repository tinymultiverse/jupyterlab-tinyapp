"""
Copyright 2024 BlackRock, Inc.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""

import hashlib
import json
import logging
import os
import random
import re
from enum import Enum
import string
import subprocess
from typing import Optional, Awaitable
import psutil
from nbconvert import PythonExporter
import asyncio
import aiofiles
import tornado
from tornado import httpclient, locks
from jupyter_server.base.handlers import APIHandler
from jupyter_server.utils import url_path_join
import nbformat
import nest_asyncio
from traitlets.config import Application
import re
from distutils.util import strtobool
from .generation.constants import StreamDestination
from .generation.generator import MockStreamingGenerator, OpenAIStreamingGenerator
from .generation.streaming import StreamParser

app = Application.instance()
logger = logging.getLogger(app.log.name)

nest_asyncio.apply()

# Global vars
logs_result = ''
logs_result_lock = asyncio.Lock() # Asyncio lock is needed since logs_result is maniuplated inside asyncio loop
execution_id = ''
execution_id_lock = locks.Lock()
execution_lock = locks.Lock() # Only the coroutine/task with this lock can run app server
app_process = None
pip_process = None
read_app_logs_task = None
read_pip_logs_task = None
watch_file_change_task = None
code_generator = None

class EnumWithLookup(Enum):
    @classmethod
    def has_value(cls, value: str) -> bool:
        return value in cls._value2member_map_
    @classmethod
    # This function assumes enum key is equals to enum value case insensitive.
    def get_order(cls, mem: str) -> int:
        for index, member in enumerate(cls._member_names_):
            if member.lower() == mem:
                return index + 1
        return 0 # Unknown member

class AppType(EnumWithLookup):
    STREAMLIT = 'streamlit'
    DASH = 'dash'

class PublishType(EnumWithLookup):
    PREVIEW = 'preview'
    PUBLICATION = 'publication'

class TinyAppServerEndpoint(Enum):
    CREATE = '/v1/app'
    LIST = '/v1/apps'
    DELETE = '/v1/app'
    UPDATE = '/v1/app'
    LOGS= '/v1/app-logs'

MOCK_AI_STREAM = strtobool(os.getenv('MOCK_AI_STREAM', "false"))
AI_ENABLED = strtobool(os.getenv('AI_ENABLED', "true"))

if AI_ENABLED:
    if MOCK_AI_STREAM:
        code_generator = MockStreamingGenerator(logger)
    else:
        code_generator = OpenAIStreamingGenerator(logger)

# Constants
HEALTH_CHECK = '/healthz'
REQUIREMENTS = 'requirements.txt'
REQUIREMENTS_STUB = [
    line + '\n' for line in [
        '# Add requirements for your application below'
    ]]

PIP_INSTALL_TIMEOUT = 600 # Seconds
REQUEST_TIMEOUT = 10 # Seconds
APP_PREVIEW_PORT = os.getenv('APP_PREVIEW_PORT', '8002')
APP_PREVIEW_HOST = os.getenv('APP_PREVIEW_HOST', f'http://127.0.0.1:{APP_PREVIEW_PORT}')

CONVERTED_PYTHON_APP_NAME = '.tinyapp_autogenerated'
CONVERTED_PYTHON_FILE_NAME = CONVERTED_PYTHON_APP_NAME + '.py'

NOTEBOOK_NAME = 'main.ipynb'
NOTEBOOK_STUB = """\
# Write your streamlit or dash code here"""

TINY_APP_SERVER_URL = os.environ['TINY_APP_SERVER_URL']
APP_PREVIEW_BASE_URL = os.getenv('APP_PREVIEW_BASE_URL', '/tinyapp/app-preview')
JUPYTER_BASE_URL = os.getenv('JUPYTER_BASE_URL', '')
CONDA_ENV_DIR = os.getenv('CONDA_ENV_DIR', '/opt/conda')
VALIDATE_SSL = os.getenv('VALIDATE_SSL', 'true').lower() == 'true'
# Ideally should be same as image under which this extension is running to ensure
# a consistent environment.
TINY_APP_IMAGE = os.getenv('TINY_APP_IMAGE', '')
BASE_DIR = os.getcwd()

# VOLUME_CLAIM_NAME will be mounted on published app container. It is assumed that
# BASE_DIR and VOLUME_CLAIM_NAME refer to same file system - otherwise files
# developed/previewed in this extension will not be available in published app container.
VOLUME_CLAIM_NAME = os.environ['VOLUME_CLAIM_NAME']
VOLUME_CLAIM_MOUNT_PATH = os.environ['VOLUME_CLAIM_MOUNT_PATH']
VOLUME_CLAIM_SUB_PATH = os.getenv('VOLUME_CLAIM_SUB_PATH', '')

class CustomAPIHandler(APIHandler):

    def data_received(self, chunk: bytes) -> Optional[Awaitable[None]]:
        pass

    def _return_error(self, status, message):
        self.set_status(status)
        self.finish(json.dumps({
            'message': message
        }))

class PreviewLogsHandler(CustomAPIHandler):
    @tornado.web.authenticated
    def get(self):
        logger.info('Received request to PreviewLogsHandler')

        self.finish(json.dumps({
            'data': {
                'logs': logs_result
            }
        }))

class LogsHandler(CustomAPIHandler):
    @tornado.web.authenticated
    async def get(self):
        logger.info(f'Received request to LogsHandler')
        app_name = self.get_arguments('app_name')[0]

        api_route = f'{TINY_APP_SERVER_URL}{TinyAppServerEndpoint.LOGS.value}?app_id={app_name}'
        logger.info(f'Using api route: {api_route}')

        http_client = httpclient.AsyncHTTPClient()
        response = None
        try:
            response = await http_client.fetch(request=api_route, method="GET", request_timeout=REQUEST_TIMEOUT,
                                               raise_error=False, validate_cert=VALIDATE_SSL)
            response.rethrow()
        except httpclient.HTTPError as e:
            err_message = e.message
            if response != None:
                resp_json = json.loads(response.body)
                if 'message' in resp_json:
                    err_message = resp_json['message']

            logger.error(f'Http error making logs request: {str(e.code)} {err_message}')
            self._return_error(e.code, err_message)
            return
        except Exception as e:
            logger.error(f'General error making logs request: {str(e)}')
            self._return_error(500, 'cannot communicate with tiny app server')
            return

        logger.info('Successful logs request')

        self.finish(json.dumps({
            'data': {
                'logs': json.loads(response.body.decode('utf-8'))['logs']
            }
        }))

async def convert_ipynb_to_py(nb_content: str, dst_path: str):
    if not nb_content:
        logger.info('Notebook file is empty. skipping conversion')
        return

    # Parse the .ipynb content
    notebook = nbformat.reads(nb_content, as_version=4)

    # Initialize the PythonExporter
    exporter = PythonExporter()

    # Convert notebook to Python script
    py_code, _ = exporter.from_notebook_node(notebook)

    # Write the Python code to a .py file
    async with aiofiles.open(dst_path, 'w') as py_file:
        await py_file.write(py_code)

# Watch for file change in notebook_path and convert to .py file at python_file_path.
async def watch_file_change(notebook_path, python_file_path):
    logger.info(f'Schedule watcher for file change at {notebook_path}')

    file_hash = ''

    while True:
        async with aiofiles.open(notebook_path, 'rb') as nb_file:
            nb_content_bytes = await nb_file.read()

        md5 = hashlib.md5()
        md5.update(nb_content_bytes)
        file_hash_new = md5.hexdigest()

        # Ignore hash diff during first iteration
        if file_hash == '':
            file_hash = file_hash_new

        if file_hash != file_hash_new:
            logger.debug('File change detected')
            file_hash = file_hash_new
            nb_content = nb_content_bytes.decode('utf-8')
            await convert_ipynb_to_py(nb_content, python_file_path)
        
        await asyncio.sleep(1)

async def read_logs(stream):
    global logs_result
    global logs_result_lock

    async with logs_result_lock:
        logger.debug('Acquired logs lock')
        while True:
            line = await stream.readline()
            if line:
                line_str = line.decode('utf-8')
                logs_result = logs_result + line_str
            else:
                break

async def clear_logs():
    global logs_result
    global logs_result_lock

    logger.debug('Acquiring lock to clear logs')
    async with logs_result_lock:
        logger.debug('Acquired logs lock')
        logs_result = ''

async def reset_logs():
    global read_app_logs_task
    global read_pip_logs_task
    
    # Cancel previous logs tasks
    if read_app_logs_task is not None:
        read_app_logs_task.cancel()
    if read_pip_logs_task is not None:
        read_pip_logs_task.cancel()

    # Clear logs
    await asyncio.get_running_loop().create_task(clear_logs())

def get_child_pids(pid):
    pids = []
    try:
        process = psutil.Process(pid)
        children = process.children(recursive=True)
        for child in children:
            pids.append(child.pid)
    except psutil.NoSuchProcess:
        logger.debug('No such process')

    return pids

async def wait_for_terminate(pids, timeout=60):
    logger.info('Waiting for process(es) to terminate')
    start_time = asyncio.get_event_loop().time()
    while True:
        if all(not psutil.pid_exists(pid) for pid in pids):
            return True
        if asyncio.get_event_loop().time() - start_time >= timeout:
            return False
        await asyncio.sleep(1)

def terminate_process(pid):
    try:
        process = psutil.Process(pid)
        process.kill()
    except psutil.NoSuchProcess:
        logger.debug('No such process')

def get_app_process_ids():
    # Find app process(es) info
    pid_to_ppid = {}
    for process in psutil.process_iter(['pid', 'cmdline', 'ppid']):
        pid = process.info['pid']
        cmd = process.info['cmdline']
        ppid = process.info['ppid']

        if cmd != None and CONVERTED_PYTHON_APP_NAME in ','.join(cmd):
            pid_to_ppid[pid] = ppid
    
    if len(pid_to_ppid) == 0:
        return None, []
    
    # Find parent process
    parent_pid = None
    for pid, ppid in pid_to_ppid.items():
        if ppid not in pid_to_ppid:
            parent_pid = pid
            break
    
    child_pids = list(pid_to_ppid.keys())
    child_pids.remove(parent_pid)
    
    return parent_pid, child_pids

# Wait until process starts running
async def wait_for_process_start(pid, timeout):
    start_time = asyncio.get_event_loop().time()
    while True:
        try:
            process = psutil.Process(pid)
            if process.is_running():
                return True
        except psutil.NoSuchProcess:
            logger.debug('No such process')
        
        if asyncio.get_event_loop().time() - start_time >= timeout:
            return False

        await asyncio.sleep(0.2)

class PreviewHandler(CustomAPIHandler):
    @tornado.web.authenticated
    async def post(self):
        global execution_id

        input_data = self.get_json_body()
        logger.info(f'Received request to PreviewHandler with input:\n{input_data}')

        # Validate notebook path
        notebook_path = input_data['notebookPath']
        if not os.path.exists(notebook_path):
            logger.error('Notebook file was not found')
            self._return_error(400, 'you need notebook file to preview an app')
            return
        
        if not notebook_path.endswith('.ipynb'):
            logger.error('Notebook path must end with .ipynb: ' + notebook_path)
            self._return_error(400, 'notebook path must end with .ipynb: ' + notebook_path)
            return
        
        logger.info(f'Notebook path: {notebook_path}')

        # Validate app type
        app_type = input_data['appType']
        if not AppType.has_value(app_type):
            self._return_error(400, f'{app_type} is not a valid app type')
            return

        local_execution_id = ''.join(random.choices(string.ascii_letters, k=7))

        logger.debug('Acquiring execution id lock to modify')
        with await execution_id_lock.acquire():
            execution_id = local_execution_id
            logger.debug(f'Updated execution id to {execution_id}')

        # Clean up previous pip process.
        # Do this without execution lock to potentially terminate long-running pip process
        # from previous preview handler.
        if pip_process is not None:
            if not await self.terminatePipProcess():
                return

        logger.debug('Acquiring execution lock to run app server')
        with await execution_lock.acquire():
            logger.debug('Acquired execution lock')

            # Because we are terminating pip process without execution lock, there is a small
            # chance of race condition to execution_lock.acquire() after pip process terminates.
            # To mitigate, we use execution id/lock to cancel run if subsequent preview request
            # has been made and has updated the execution id.
            if not await self.checkExecutionId(local_execution_id):
                return

            await reset_logs()

            # Install requirements.txt
            app_src_directory = os.path.dirname(notebook_path)
            if not await self.install_requirements(app_src_directory):
                return

            # Converting .ipynb app file to .py
            await self.handleAppFileConvert(app_src_directory, notebook_path)

            # Clean up previous app process
            if not await self.terminateAppProcess():
                return
            
            if not await self.startAppServer(app_type, app_src_directory):
                return

            self.finish(json.dumps({
                'data': {
                    'appURL': f'{APP_PREVIEW_HOST}{APP_PREVIEW_BASE_URL}'
                }
            }))

    # proc_type is either 'app' or 'pip' - used purely for logging purpose.
    async def terminateProcess(self, proc_type, parent_pid, child_pids):
        # If there are child processes, terminate them first.
        if len(child_pids) > 0:
            # Terminate child processes before parent process because sometimes terminating parent
            # process first leaves child process defunct (zombie).
            for c_pid in child_pids:
                logger.info(f'Terminating child process ({proc_type}) {str(c_pid)}')
                terminate_process(c_pid)
            if not await wait_for_terminate(child_pids):
                logger.error(f'Timeout occured while waiting for child processes ({proc_type}) to terminate')
                self._return_error(500, f'error while terminating {proc_type} process')
                return False

        if parent_pid is not None:
            logger.info(f'Terminating parent process ({proc_type}) {str(parent_pid)}')
            terminate_process(parent_pid)
            if not await wait_for_terminate([parent_pid]):
                logger.error(f'Timeout occured while waiting for parent process ({proc_type}) to terminate')
                self._return_error(500, f'error while terminating {proc_type} process')
                return False
        
        return True

    async def terminatePipProcess(self):
        child_pids = get_child_pids(pip_process.pid)
        return await self.terminateProcess('pip', pip_process.pid, child_pids)

    # Install requirements.txt if it exists. Returns False if there's an error.
    async def install_requirements(self, app_src_directory):
        global pip_process
        global read_pip_logs_task

        # Check if requirements file exists
        requirements_path = f'{app_src_directory}/requirements.txt'
        if not os.path.exists(requirements_path):
            logger.info('Requirements file does not exist')
            return True

        # Install requirements
        logger.info('Installing requirements.txt')
        pip_path = os.path.join(CONDA_ENV_DIR, 'bin/pip')
        pip_process = await asyncio.create_subprocess_shell(f'{pip_path} install -r {requirements_path}',
                    stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.STDOUT, universal_newlines=False)
        
        logger.debug('Acquiring lock to continously read logs for pip process')
        read_pip_logs_task = asyncio.create_task(read_logs(pip_process.stdout))

        try:
            await asyncio.wait_for(asyncio.gather(pip_process.wait(), read_pip_logs_task),
                                PIP_INSTALL_TIMEOUT)
        except asyncio.TimeoutError:
            logger.error(f'Timeout occured while installing requirements.txt')

            if pip_process.returncode is None:
                await self.terminatePipProcess(pip_process.pid)
            
            self._return_error(500, f'error while installing requirements.txt')
            return False

        if pip_process.returncode != 0:
            logger.error(f'Non-zero returncode while installing requirements.txt: {str(pip_process.returncode)}')
            self._return_error(500, f'error while installing requirements.txt')
            return False

        return True

    # Convert .ipynb to .py file and set up a watcher to do the same every time there's change.
    async def handleAppFileConvert(self, app_src_directory, notebook_path):
        global watch_file_change_task

        python_file_path = os.path.join(app_src_directory, CONVERTED_PYTHON_FILE_NAME)
        
        logger.info('Converting .ipynb file to .py file')
        async with aiofiles.open(notebook_path, 'r') as nb_file:
            nb_content = await nb_file.read()
        await convert_ipynb_to_py(nb_content, python_file_path)

        if watch_file_change_task is not None:
            watch_file_change_task.cancel()
        
        # Convert .ipynb to .py every time there's change
        watch_file_change_task = asyncio.create_task(watch_file_change(notebook_path, python_file_path))

    async def terminateAppProcess(self):
        parent_pid, child_pids = get_app_process_ids()
        return await self.terminateProcess('app', parent_pid, child_pids)

    async def startAppServer(self, app_type, app_src_directory):
        global app_process
        global read_app_logs_task

        logger.info(f'Starting {app_type} server')
        python_path = os.path.join(CONDA_ENV_DIR, 'bin/python')

        if app_type == AppType.STREAMLIT.value:
            app_process = await asyncio.create_subprocess_shell(f'{python_path} -u -m streamlit run '
                        + f'{CONVERTED_PYTHON_FILE_NAME} --server.port {APP_PREVIEW_PORT} '
                        + f'--server.headless true --server.baseUrlPath {JUPYTER_BASE_URL}{APP_PREVIEW_BASE_URL} '
                        + '--server.runOnSave true', stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.STDOUT,
                        cwd=app_src_directory)

        elif app_type == AppType.DASH.value:
            dash_env = dict(os.environ)
            dash_env['DASH_URL_BASE_PATHNAME'] = f'{JUPYTER_BASE_URL}{APP_PREVIEW_BASE_URL}/'
            app_process = await asyncio.create_subprocess_shell(f'{python_path} -u -m gunicorn --reload --reuse-port '
                        + f'--threads 4 {CONVERTED_PYTHON_APP_NAME}:server -b :{APP_PREVIEW_PORT} ',
                        stdout=subprocess.PIPE, stderr=asyncio.subprocess.STDOUT, cwd=app_src_directory, env=dash_env)

        asyncio.create_task(app_process.wait())
        logger.debug('Acquiring lock to continously read logs for app process')
        read_app_logs_task = asyncio.create_task(read_logs(app_process.stdout))

        # Wait until app process is in running state
        app_process_started = await wait_for_process_start(app_process.pid, 10)
        if not app_process_started:
            logger.error('Timeout occured while waiting for app process to start')
            self._return_error(500, f'error while starting {app_type} server')
            return False

        return True

    # Check if given execution id is still equal to global execution id.
    async def checkExecutionId(self, local_execution_id):
        logger.debug('Acquiring execution id lock to read')
        with await execution_id_lock.acquire():
            logger.debug('Acquired execution id lock.')
            if execution_id != local_execution_id:
                logger.info(f'Execution id changed from {local_execution_id} to {execution_id}. Exiting.')
                self._return_error(400, 'execution id changed. exiting')
                return False
        return True

class PublishHandler(CustomAPIHandler):
    @tornado.web.authenticated
    async def post(self):
        input_data = self.get_json_body()
        logger.info(f'Received request to PublishHandler with input:\n{input_data}')

        # Validate notebook path exists
        notebook_path = input_data['notebookPath']
        if not os.path.exists(notebook_path):
            logger.error('Notebook file was not found')
            self._return_error(400, 'you need notebook file to preview an app')
            return
        
        # Validate notebook file extension
        if not notebook_path.endswith('.ipynb'):
            logger.error('Notebook path must end with .ipynb: ' + notebook_path)
            self._return_error(400, 'notebook path must end with .ipynb: ' + notebook_path)
            return

        # Check if requirements file exists
        app_src_directory = os.path.dirname(notebook_path)
        requirements_path = f'{app_src_directory}/requirements.txt'
        if not os.path.exists(requirements_path):
            logger.info('Requirements file does not exist')

        # Validate app type
        app_type = input_data['appType']
        app_type_order = AppType.get_order(app_type)
        if app_type_order < 1:
            self._return_error(400, f'{app_type} is not a valid app type')
            return

        api_route = f'{TINY_APP_SERVER_URL}{TinyAppServerEndpoint.CREATE.value}'

        appDetail = {
            'name': input_data['appTitle'],
            'description': input_data['appDescription'],
            'mainFilePath': notebook_path,
            'appType': app_type_order, # 1 for streamlit, 2 for dash
            'sourceType': 2, # File system
            'image': TINY_APP_IMAGE,
            'volumeClaims': [
                {
                    'name': VOLUME_CLAIM_NAME,
                    'subPath': VOLUME_CLAIM_SUB_PATH,
                    'mountPath': VOLUME_CLAIM_MOUNT_PATH
                }
            ],
            'mainVolumeClaimName': VOLUME_CLAIM_NAME
        }

        # TODO Mount additional volume claims

        headers = {
            'Content-Type': 'application/json'
        }
        request_data = {
            'appDetail': appDetail
        }

        logger.info(f'Making request to tiny app server to publish app\n{request_data}')
        http_client = httpclient.AsyncHTTPClient()
        response = None
        try:
            response = await http_client.fetch(request=api_route, method="POST", headers=headers,
                                               body=json.dumps(request_data), request_timeout=REQUEST_TIMEOUT, raise_error=False, validate_cert=VALIDATE_SSL)
            response.rethrow()
        except httpclient.HTTPError as e:
            err_message = e.message
            if response != None:
                resp_json = json.loads(response.body)
                if 'message' in resp_json:
                    err_message = resp_json['message']

            logger.error(f'Http error while publishing app: {str(e.code)} {err_message}')
            self._return_error(e.code, err_message)
            return
        except Exception as e:
            logger.error(f'General error while publishing app: {str(e)}')
            self._return_error(500, 'cannot communicate with tiny app server')
            return

        response_data = json.loads(response.body.decode('utf-8')).get('appRelease')
        logger.info(f'Successful publish request')

        self.finish(json.dumps({
            'data': response_data
        }))

def valid_app_name(app_name: str) -> bool:
    """
    validates an app name so this it conforms to standard directory naming protocols (although with no spaces allowed)
    :param app_name: the given name for the application
    :return:
    """
    return bool(re.match("^[A-Za-z0-9_-]*$", app_name))

class NewAppDirectoryHandler(CustomAPIHandler):
    @tornado.web.authenticated
    def post(self):
        input_data = self.get_json_body()
        directory_name = input_data['appDirectory']

        logger.info(f'Received request to NewAppDirectoryHandler with input:\n{input_data}')

        if not valid_app_name(directory_name):
            logger.info('Invalid app directory name. Must be alphanumeric with no special characters other than "-" or "_"')
            self._return_error(400, 'invalid app directory name. Must be alphanumeric with no special characters other than "-" or "_"')
            return

        app_src_directory = os.path.join(BASE_DIR, directory_name)
        if os.path.exists(app_src_directory):
            logger.info('Directory already exists. cannot create app directory with that name')
            self._return_error(400, 'an app directory with that name already exists. please choose another name')
            return
        else:
            logger.info(f'Created new app directory at {app_src_directory}')

        os.mkdir(app_src_directory)

        # Create requirements file
        requirements_path = os.path.join(app_src_directory, REQUIREMENTS)
        with open(requirements_path, 'w') as requirements:
            requirements.writelines(REQUIREMENTS_STUB)
        
        nb = nbformat.v4.new_notebook()
        nb['cells'] = [nbformat.v4.new_code_cell(NOTEBOOK_STUB)]

        notebook_path = os.path.join(app_src_directory, NOTEBOOK_NAME)
        nbformat.write(nb, notebook_path)

        logger.info('App workspace successfully initialized')

        self.finish(json.dumps({
            'data': {
                'message': 'new app directory created successfully',
                'path': app_src_directory
            }
        }))

# Simple WebSocket Handler for AI-powered app generation and streaming
class GenerateAppHandler(tornado.websocket.WebSocketHandler):
    def check_origin(self, origin):
        return True  # Allow connections from any origin

    def open(self):
        logger.debug("WebSocket opened")

    def on_close(self):
        logger.info("WebSocket closed")
    
    async def on_message(self, message):
        logger.info('Received message to GenerateAppHandler')

        # TODO Input validation
        input_data = json.loads(message)
        logger.debug(f'GenerateAppHandler input:\n{input_data}')

        notebook_path_relative = input_data['notebookPath']
        prompt = input_data['prompt']
        image = input_data['image']

        notebook_path = os.path.join(BASE_DIR, notebook_path_relative)
        app_src_directory = os.path.dirname(notebook_path)

        if not os.path.exists(notebook_path):
            logger.error(f'Notebook path "{notebook_path}" was not found - cannot generate app')
            self._return_error(412, 'you need a notebook file to generate an app')
            return

        # Start the asynchronous streaming process
        logger.info("Start streaming generation...")
        await self.stream_response(app_src_directory, prompt, image)

    async def stream_response(self, app_src_directory, prompt, image):
        full_msg  = ""
        for chunk in self.generate_response(app_src_directory, prompt, image):
            full_msg += chunk
            self.write_message(chunk)
        logger.debug(f"Entire streamed response: {full_msg}")

    def generate_response(self, app_src_directory, prompt, image):
        stream = code_generator.create_stream(prompt, image)

        generated_requirements = ""
        stream_destination = StreamDestination.NOTEBOOK

        stream_parser = StreamParser(logger, stream)
        for chunk in stream_parser:
            logger.debug(f"Chunk in generate response: {chunk}")
            match chunk:
                case "<<deps>>":
                    stream_destination = StreamDestination.REQUIREMENTS
                    continue
                case "<</deps>>":
                    # Write the generated requirements
                    logger.debug(f"Writing requirements.txt file: {generated_requirements}")
                    requirements_path = os.path.join(app_src_directory, REQUIREMENTS)
                    if len(generated_requirements) > 0:
                        with open(requirements_path, 'w') as requirements:
                            requirements.writelines(generated_requirements.strip())
                    stream_destination = StreamDestination.NOTEBOOK
                    continue

            # Route the stream to the appropriate destination
            match (stream_destination):
                case StreamDestination.NOTEBOOK:
                    logger.debug(f"Emitting chunk: {chunk}")
                    yield chunk
                case StreamDestination.REQUIREMENTS:
                    logger.debug(f"Adding chunk to requirements: {chunk}")
                    generated_requirements += chunk

# Make sure not to expose sensitive env vars
class GetEnvVarsHandler(CustomAPIHandler):
    @tornado.web.authenticated
    async def get(self):
        logger.info('Received request to GetEnvVarsHandler')

        self.finish(json.dumps({
            'ai_enabled': AI_ENABLED,
        }))

class ListAppsHandler(CustomAPIHandler):
    @tornado.web.authenticated
    async def get(self):
        logger.info('Received request to ListAppsHandler')

        api_route = f'{TINY_APP_SERVER_URL}{TinyAppServerEndpoint.LIST.value}'
        logger.info(f'Making request to tiny app server to list apps: {api_route}')

        http_client = httpclient.AsyncHTTPClient()
        response = None
        try:
            response = await http_client.fetch(request=api_route, method="GET", request_timeout=REQUEST_TIMEOUT,          
                                               raise_error=False, validate_cert=VALIDATE_SSL)
            response.rethrow()
        except httpclient.HTTPError as e:
            err_message = e.message
            if response != None:
                resp_json = json.loads(response.body)
                if 'message' in resp_json:
                    err_message = resp_json['message']
            
            logger.error(f'Http error listing apps: {str(e.code)} {err_message}')
            self._return_error(e.code, err_message)
            return
        except Exception as e:
            logger.error(f'General error while listing apps: {str(e)}')
            self._return_error(500, 'cannot communicate with tiny app server')
            return

        logger.info('Successful list request')

        self.finish(json.dumps({
            'data': json.loads(response.body.decode('utf-8'))
        }))


class DeleteAppHandler(CustomAPIHandler):
    @tornado.web.authenticated
    async def post(self):
        input_data = self.get_json_body()
        logger.info(f'Received request DeleteAppHandler with input:\n{input_data}')

        app_id = input_data['appId']

        api_route = f'{TINY_APP_SERVER_URL}{TinyAppServerEndpoint.DELETE.value}'
        api_route_with_params = api_route + f'?appId={app_id}'

        logger.info(f'Route for delete is: {api_route_with_params}')

        http_client = httpclient.AsyncHTTPClient()
        response = None
        try:
            response = await http_client.fetch(request=api_route_with_params, method="DELETE",
                                request_timeout=REQUEST_TIMEOUT, raise_error=False, validate_cert=VALIDATE_SSL)
            response.rethrow()
        except httpclient.HTTPError as e:
            err_message = e.message
            if response != None:
                resp_json = json.loads(response.body)
                if 'message' in resp_json:
                    err_message = resp_json['message']

            logger.error(f'Http error deleting app: {str(e.code)} {err_message}')
            self._return_error(e.code, err_message)
            return
        except Exception as e:
            logger.error(f'General error while deleting app: {str(e)}')
            self._return_error(500, 'cannot communicate with tiny app server')
            return

        logger.info('Successfully deleted app')

        self.finish(json.dumps({
            'data': {}
        }))


class PingHandler(CustomAPIHandler):
    @tornado.web.authenticated
    async def post(self):
        input_data = self.get_json_body()
        logger.info(f'Received request PingHandler with input:\n{input_data}')
        
        ok = False
        api_route = input_data.get('url', '').strip('/') + HEALTH_CHECK

        logger.info(f'Ping route is: {api_route}')

        http_client = httpclient.AsyncHTTPClient()
        response = None
        try:
            response = await http_client.fetch(request=api_route, method='GET', 
                                               raise_error=False, request_timeout=REQUEST_TIMEOUT, validate_cert=VALIDATE_SSL)
        except Exception as e:
            logger.info(f'Unable to ping: {str(e)}')
        
        if response != None:
            logger.info(f'Ping response: {response.code} {response.reason}')
            if response.code >= 200 and response.code < 300:
                ok = True

        self.finish(json.dumps({
            'data': {
                'ok': ok,
            }
        }))


def setup_handlers(web_app):
    base_url = web_app.settings['base_url']
    tiny_app_subpath = 'tinyapp'
    host_pattern = '.*$'

    handlers = [
        (url_path_join(base_url, tiny_app_subpath, 'new_app_dir'), NewAppDirectoryHandler),
        (url_path_join(base_url, tiny_app_subpath, 'preview'), PreviewHandler),
        (url_path_join(base_url, tiny_app_subpath, 'publish'), PublishHandler),
        (url_path_join(base_url, tiny_app_subpath, 'list_apps'), ListAppsHandler),
        (url_path_join(base_url, tiny_app_subpath, 'get_env_vars'), GetEnvVarsHandler),
        (url_path_join(base_url, tiny_app_subpath, 'delete_app'), DeleteAppHandler),
        (url_path_join(base_url, tiny_app_subpath, 'logs'), LogsHandler),
        (url_path_join(base_url, tiny_app_subpath, 'preview_logs'), PreviewLogsHandler),
        (url_path_join(base_url, tiny_app_subpath, 'ping'), PingHandler)
    ]

    if AI_ENABLED:
        handlers.append((url_path_join(base_url, tiny_app_subpath, 'generate_app'), GenerateAppHandler))

    # Log endpoints
    for handler in handlers:
        logger.info(f'Registered handler for endpoint "{handler[0]}"')
    web_app.add_handlers(host_pattern, handlers)
